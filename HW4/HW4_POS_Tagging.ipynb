{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 4 - Neural POS Tagger\n",
    "\n",
    "In this exercise, you are going to build a set of deep learning models on part-of-speech (POS) tagging using Tensorflow and Keras. Tensorflow is a deep learning framwork developed by Google, and Keras is a frontend library built on top of Tensorflow (or Theano, CNTK) to provide an easier way to use standard layers and networks.\n",
    "\n",
    "To complete this exercise, you will need to build deep learning models for POS tagging in Thai using NECTEC's ORCHID corpus. You will build one model for each of the following type:\n",
    "\n",
    "- Neural POS Tagging with Word Embedding using Fixed / non-Fixed Pretrained weights\n",
    "- Neural POS Tagging with Viterbi / Marginal CRF\n",
    "\n",
    "Pretrained word embeddding are already given for you to use (albeit, a very bad one). Optionally, you can use your best pretrained word embeddding from previous exercise.\n",
    "\n",
    "We also provide the code for data cleaning, preprocessing and some starter code for keras in this notebook but feel free to modify those parts to suit your needs. You can also complete this exercise using only Tensorflow (without using Keras). Feel free to use additional libraries (e.g. scikit-learn) as long as you have a model for each type mentioned above.\n",
    "\n",
    "### Don't forget to shut down your instance on Gcloud when you are not using it ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use POS data from [ORCHID corpus](https://www.nectec.or.th/corpus/index.php?league=pm), which is a POS corpus for Thai language.\n",
    "A method used to read the corpus into a list of sentences with (word, POS) pairs have been implemented already. The example usage has shown below.\n",
    "We also create a word vector for unknown word by random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from data.orchid_corpus import get_sentences\n",
    "import numpy as np\n",
    "import numpy.random\n",
    "import keras.preprocessing\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('การ', 'FIXN'), ('ประชุม', 'VACT'), ('ทาง', 'NCMN'), ('วิชาการ', 'NCMN'), ('<space>', 'PUNC'), ('ครั้ง', 'CFQC'), ('ที่ 1', 'DONM')]\n"
     ]
    }
   ],
   "source": [
    "unk_emb = np.random.randn(32)\n",
    "train_data = get_sentences('train')\n",
    "test_data = get_sentences('test')\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load pretrained weight embedding using pickle. The pretrained weight is a dictionary which map a word to its embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "fp = open('basic_ff_embedding.pt', 'rb')\n",
    "embeddings = pickle.load(fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The given code below generates an indexed dataset(each word is represented by a number) for training and testing data. The index 0 is reserved for padding to help with variable length sequence. (Additionally, You can read more about padding here [https://machinelearningmastery.com/data-preparation-variable-length-input-sequences-sequence-prediction/])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_to_idx ={}\n",
    "idx_to_word ={}\n",
    "label_to_idx = {}\n",
    "for sentence in train_data:\n",
    "    for word,pos in sentence:\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = len(word_to_idx)+1\n",
    "            idx_to_word[word_to_idx[word]] = word\n",
    "        if pos not in label_to_idx:\n",
    "            label_to_idx[pos] = len(label_to_idx)+1\n",
    "word_to_idx['UNK'] = len(word_to_idx)\n",
    "\n",
    "n_classes = len(label_to_idx.keys())+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is tweaked a little from the demo, word2features will return word index instead of features, and sent2labels will return a sequence of word indices in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2features(sent, i, emb):\n",
    "    word = sent[i][0]\n",
    "    if word in word_to_idx :\n",
    "        return word_to_idx[word]\n",
    "    else :\n",
    "        return word_to_idx['UNK']\n",
    "\n",
    "def sent2features(sent, emb_dict):\n",
    "    return np.asarray([word2features(sent, i, emb_dict) for i in range(len(sent))])\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return np.asarray([label_to_idx[label] for (word, label) in sent],dtype='int32')\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [word for (word, label) in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 29, 327,   5, 328])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2features(train_data[100],embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create train and test dataset, then we use keras to post-pad the sequence to max sequence with 0. Our labels are changed to a one-hot vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 349 ms, sys: 65 µs, total: 349 ms\n",
      "Wall time: 349 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train = np.asarray([sent2features(sent, embeddings) for sent in train_data])\n",
    "y_train = [sent2labels(sent) for sent in train_data]\n",
    "x_test = [sent2features(sent, embeddings) for sent in test_data]\n",
    "y_test = [sent2labels(sent) for sent in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train=keras.preprocessing.sequence.pad_sequences(x_train, maxlen=None, dtype='int32', padding='post', truncating='pre', value=0.)\n",
    "y_train=keras.preprocessing.sequence.pad_sequences(y_train, maxlen=None, dtype='int32', padding='post', truncating='pre', value=0.)\n",
    "x_test=keras.preprocessing.sequence.pad_sequences(x_test, maxlen=102, dtype='int32', padding='post', truncating='pre', value=0.)\n",
    "y_temp =[]\n",
    "for i in range(len(y_train)):\n",
    "    y_temp.append(np.eye(n_classes)[y_train[i]][np.newaxis,:])\n",
    "y_train = np.asarray(y_temp).reshape(-1,102,n_classes)\n",
    "del(y_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 29 327   5 328   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0] (18500, 102)\n",
      "[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] (18500, 102, 48)\n"
     ]
    }
   ],
   "source": [
    "print(x_train[100],x_train.shape)\n",
    "print(y_train[100][3],y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our output from keras is a distribution of problabilities on all possible label. outputToLabel will return an indices of maximum problability from output sequence.\n",
    "\n",
    "evaluation_report is the same as in the demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def outputToLabel(yt,seq_len):\n",
    "    out = []\n",
    "    for i in range(0,len(yt)):\n",
    "        if(i==seq_len):\n",
    "            break\n",
    "        out.append(np.argmax(yt[i]))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "def evaluation_report(y_true, y_pred):\n",
    "    # retrieve all tags in y_true\n",
    "    tag_set = set()\n",
    "    for sent in y_true:\n",
    "        for tag in sent:\n",
    "            tag_set.add(tag)\n",
    "    for sent in y_pred:\n",
    "        for tag in sent:\n",
    "            tag_set.add(tag)\n",
    "    tag_list = sorted(list(tag_set))\n",
    "    \n",
    "    # count correct points\n",
    "    tag_info = dict()\n",
    "    for tag in tag_list:\n",
    "        tag_info[tag] = {'correct_tagged': 0, 'y_true': 0, 'y_pred': 0}\n",
    "\n",
    "    all_correct = 0\n",
    "    all_count = sum([len(sent) for sent in y_true])\n",
    "    for sent_true, sent_pred in zip(y_true, y_pred):\n",
    "        for tag_true, tag_pred in zip(sent_true, sent_pred):\n",
    "            if tag_true == tag_pred:\n",
    "                tag_info[tag_true]['correct_tagged'] += 1\n",
    "                all_correct += 1\n",
    "            tag_info[tag_true]['y_true'] += 1\n",
    "            tag_info[tag_pred]['y_pred'] += 1\n",
    "    accuracy = (all_correct / all_count) * 100\n",
    "            \n",
    "    # summarize and make evaluation result\n",
    "    eval_list = list()\n",
    "    for tag in tag_list:\n",
    "        eval_result = dict()\n",
    "        eval_result['tag'] = tag\n",
    "        eval_result['correct_count'] = tag_info[tag]['correct_tagged']\n",
    "        precision = (tag_info[tag]['correct_tagged']/tag_info[tag]['y_pred'])*100 if tag_info[tag]['y_pred'] else '-'\n",
    "        recall = (tag_info[tag]['correct_tagged']/tag_info[tag]['y_true'])*100 if (tag_info[tag]['y_true'] > 0) else 0\n",
    "        eval_result['precision'] = precision\n",
    "        eval_result['recall'] = recall\n",
    "        eval_result['f_score'] = (2*precision*recall)/(precision+recall) if (type(precision) is float and recall > 0) else '-'\n",
    "        \n",
    "        eval_list.append(eval_result)\n",
    "\n",
    "    eval_list.append({'tag': 'accuracy=%.2f' % accuracy, 'correct_count': '', 'precision': '', 'recall': '', 'f_score': ''})\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(eval_list)\n",
    "    df = df[['tag', 'precision', 'recall', 'f_score', 'correct_count']]\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, Reshape, Activation, Input, Dense,GRU,Reshape,TimeDistributed,Bidirectional,Dropout,Masking\n",
    "from keras_contrib.layers import CRF\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is this section is separated to two groups\n",
    "\n",
    "- Neural POS Tagger (4.1)\n",
    "- Neural CRF POS Tagger (4.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.1 Neural POS Tagger  (Example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a simple Neural POS Tagger as an example for you. This model dosen't use any pretrained word embbeding so it need to use Embedding layer to train the word embedding from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 102, 32)           480608    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 102, 64)           12480     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 102, 64)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 102, 48)           3120      \n",
      "=================================================================\n",
      "Total params: 496,208\n",
      "Trainable params: 496,208\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word_to_idx),32,input_length=102,mask_zero=True))\n",
    "model.add(Bidirectional(GRU(32, return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(TimeDistributed(Dense(n_classes,activation='softmax')))\n",
    "model.summary()\n",
    "adam  = Adam(lr=0.001)\n",
    "model.compile(optimizer=adam,  loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18500/18500 [==============================] - 137s - loss: 1.9013 - categorical_accuracy: 0.5459   \n",
      "Epoch 2/10\n",
      "18500/18500 [==============================] - 124s - loss: 0.4276 - categorical_accuracy: 0.9024   \n",
      "Epoch 3/10\n",
      "18500/18500 [==============================] - 123s - loss: 0.2552 - categorical_accuracy: 0.9352   \n",
      "Epoch 4/10\n",
      "18500/18500 [==============================] - 122s - loss: 0.1996 - categorical_accuracy: 0.9460   \n",
      "Epoch 5/10\n",
      "18500/18500 [==============================] - 121s - loss: 0.1720 - categorical_accuracy: 0.9522   \n",
      "Epoch 6/10\n",
      "18500/18500 [==============================] - 120s - loss: 0.1563 - categorical_accuracy: 0.9551   \n",
      "Epoch 7/10\n",
      "18500/18500 [==============================] - 120s - loss: 0.1445 - categorical_accuracy: 0.9578   \n",
      "Epoch 8/10\n",
      "18500/18500 [==============================] - 119s - loss: 0.1361 - categorical_accuracy: 0.9600   \n",
      "Epoch 9/10\n",
      "18500/18500 [==============================] - 118s - loss: 0.1289 - categorical_accuracy: 0.9617   \n",
      "Epoch 10/10\n",
      "18500/18500 [==============================] - 119s - loss: 0.1229 - categorical_accuracy: 0.9630   \n",
      "CPU times: user 55min 27s, sys: 9min 13s, total: 1h 4min 40s\n",
      "Wall time: 20min 28s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7eb31f1cf8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(x_train,y_train,batch_size=64,epochs=10,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f_score</th>\n",
       "      <th>correct_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>99.8092</td>\n",
       "      <td>99.3487</td>\n",
       "      <td>99.5784</td>\n",
       "      <td>3661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>94.7938</td>\n",
       "      <td>94.4835</td>\n",
       "      <td>94.6384</td>\n",
       "      <td>7793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>91.0462</td>\n",
       "      <td>96.5125</td>\n",
       "      <td>93.6997</td>\n",
       "      <td>16300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>99.9766</td>\n",
       "      <td>99.3654</td>\n",
       "      <td>99.6701</td>\n",
       "      <td>12840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>91.6667</td>\n",
       "      <td>98.5075</td>\n",
       "      <td>94.964</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>99.7817</td>\n",
       "      <td>87.5479</td>\n",
       "      <td>93.2653</td>\n",
       "      <td>457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>97.6374</td>\n",
       "      <td>97.4026</td>\n",
       "      <td>97.5199</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>67.4627</td>\n",
       "      <td>54.4578</td>\n",
       "      <td>60.2667</td>\n",
       "      <td>226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>56.4039</td>\n",
       "      <td>62.2283</td>\n",
       "      <td>59.1731</td>\n",
       "      <td>229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>62.6761</td>\n",
       "      <td>42.4315</td>\n",
       "      <td>50.6041</td>\n",
       "      <td>356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>83.1683</td>\n",
       "      <td>97.6744</td>\n",
       "      <td>89.8396</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>96.4504</td>\n",
       "      <td>98.377</td>\n",
       "      <td>97.4042</td>\n",
       "      <td>788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>89.338</td>\n",
       "      <td>84.8637</td>\n",
       "      <td>87.0434</td>\n",
       "      <td>3050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>93.7839</td>\n",
       "      <td>94.5527</td>\n",
       "      <td>94.1667</td>\n",
       "      <td>5190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>81.1492</td>\n",
       "      <td>71.7469</td>\n",
       "      <td>76.1589</td>\n",
       "      <td>805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>88.2822</td>\n",
       "      <td>87.5104</td>\n",
       "      <td>87.8946</td>\n",
       "      <td>2102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>96.9589</td>\n",
       "      <td>92.8082</td>\n",
       "      <td>94.8381</td>\n",
       "      <td>542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>97.7002</td>\n",
       "      <td>99.3074</td>\n",
       "      <td>98.4972</td>\n",
       "      <td>1147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>97.6471</td>\n",
       "      <td>96.2319</td>\n",
       "      <td>96.9343</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>98.6207</td>\n",
       "      <td>96.9492</td>\n",
       "      <td>97.7778</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>94.7297</td>\n",
       "      <td>92.5413</td>\n",
       "      <td>93.6227</td>\n",
       "      <td>1402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>84.9699</td>\n",
       "      <td>79.3512</td>\n",
       "      <td>82.0645</td>\n",
       "      <td>1272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>89.9261</td>\n",
       "      <td>94.8972</td>\n",
       "      <td>92.3448</td>\n",
       "      <td>1339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>91.5274</td>\n",
       "      <td>83.8251</td>\n",
       "      <td>87.5071</td>\n",
       "      <td>767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>92.381</td>\n",
       "      <td>70.46</td>\n",
       "      <td>79.9451</td>\n",
       "      <td>291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>95.7055</td>\n",
       "      <td>88.6364</td>\n",
       "      <td>92.0354</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>93.1034</td>\n",
       "      <td>82.4427</td>\n",
       "      <td>87.4494</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>29</td>\n",
       "      <td>94.704</td>\n",
       "      <td>96.2025</td>\n",
       "      <td>95.4474</td>\n",
       "      <td>304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>30</td>\n",
       "      <td>68.9655</td>\n",
       "      <td>78.4314</td>\n",
       "      <td>73.3945</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>31</td>\n",
       "      <td>69.6429</td>\n",
       "      <td>75.7282</td>\n",
       "      <td>72.5581</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>32</td>\n",
       "      <td>63.9535</td>\n",
       "      <td>61.7978</td>\n",
       "      <td>62.8571</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>33</td>\n",
       "      <td>87.5</td>\n",
       "      <td>51.4706</td>\n",
       "      <td>64.8148</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>34</td>\n",
       "      <td>87.1186</td>\n",
       "      <td>91.4591</td>\n",
       "      <td>89.2361</td>\n",
       "      <td>514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>35</td>\n",
       "      <td>71.4286</td>\n",
       "      <td>55.5556</td>\n",
       "      <td>62.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>36</td>\n",
       "      <td>100</td>\n",
       "      <td>81.25</td>\n",
       "      <td>89.6552</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>37</td>\n",
       "      <td>88.6957</td>\n",
       "      <td>100</td>\n",
       "      <td>94.0092</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>38</td>\n",
       "      <td>67.7419</td>\n",
       "      <td>53.8462</td>\n",
       "      <td>60</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>39</td>\n",
       "      <td>75.6579</td>\n",
       "      <td>82.1429</td>\n",
       "      <td>78.7671</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>40</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>41</td>\n",
       "      <td>71.4286</td>\n",
       "      <td>75</td>\n",
       "      <td>73.1707</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>42</td>\n",
       "      <td>92.8571</td>\n",
       "      <td>76.4706</td>\n",
       "      <td>83.871</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>45</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>46</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>accuracy=93.23</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tag precision   recall  f_score correct_count\n",
       "0                1   99.8092  99.3487  99.5784          3661\n",
       "1                2   94.7938  94.4835  94.6384          7793\n",
       "2                3   91.0462  96.5125  93.6997         16300\n",
       "3                4   99.9766  99.3654  99.6701         12840\n",
       "4                5   91.6667  98.5075   94.964            66\n",
       "5                6   99.7817  87.5479  93.2653           457\n",
       "6                7   97.6374  97.4026  97.5199          2025\n",
       "7                8   67.4627  54.4578  60.2667           226\n",
       "8                9   56.4039  62.2283  59.1731           229\n",
       "9               10   62.6761  42.4315  50.6041           356\n",
       "10              11   83.1683  97.6744  89.8396            84\n",
       "11              12   96.4504   98.377  97.4042           788\n",
       "12              13    89.338  84.8637  87.0434          3050\n",
       "13              14   93.7839  94.5527  94.1667          5190\n",
       "14              15   81.1492  71.7469  76.1589           805\n",
       "15              16   88.2822  87.5104  87.8946          2102\n",
       "16              17   96.9589  92.8082  94.8381           542\n",
       "17              18   97.7002  99.3074  98.4972          1147\n",
       "18              19   97.6471  96.2319  96.9343           332\n",
       "19              20   98.6207  96.9492  97.7778           286\n",
       "20              21   94.7297  92.5413  93.6227          1402\n",
       "21              22   84.9699  79.3512  82.0645          1272\n",
       "22              23   89.9261  94.8972  92.3448          1339\n",
       "23              24   91.5274  83.8251  87.5071           767\n",
       "24              25    92.381    70.46  79.9451           291\n",
       "25              26   95.7055  88.6364  92.0354           156\n",
       "26              27   93.1034  82.4427  87.4494           108\n",
       "27              29    94.704  96.2025  95.4474           304\n",
       "28              30   68.9655  78.4314  73.3945            80\n",
       "29              31   69.6429  75.7282  72.5581            78\n",
       "30              32   63.9535  61.7978  62.8571           110\n",
       "31              33      87.5  51.4706  64.8148            35\n",
       "32              34   87.1186  91.4591  89.2361           514\n",
       "33              35   71.4286  55.5556     62.5             5\n",
       "34              36       100    81.25  89.6552            13\n",
       "35              37   88.6957      100  94.0092           102\n",
       "36              38   67.7419  53.8462       60            21\n",
       "37              39   75.6579  82.1429  78.7671           115\n",
       "38              40       100      100      100           280\n",
       "39              41   71.4286       75  73.1707            15\n",
       "40              42   92.8571  76.4706   83.871            13\n",
       "41              43         0        0        -             0\n",
       "42              45         -        0        -             0\n",
       "43              46         -        0        -             0\n",
       "44  accuracy=93.23                                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 45.2 s, sys: 7.31 s, total: 52.5 s\n",
      "Wall time: 18.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#model.save_weights('/data/my_pos_no_crf.h5')\n",
    "#model.load_weights('/data/my_pos_no_crf.h5')\n",
    "y_pred=model.predict(x_test)\n",
    "ypred = [outputToLabel(y_pred[i],len(y_test[i])) for i in range(len(y_pred))]\n",
    "evaluation_report(y_test, ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.2 Neural POS Tagger - Fix Weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #TODO 1\n",
    "We would like you create a neural postagger model with keras with the pretrained word embedding as an input. The word embedding should be fixed across training time. To finish this excercise you must train the model and show the evaluation report with this model as shown in the example.\n",
    "\n",
    "(You may want to read about Keras's Masking layer)\n",
    "\n",
    "Optionally, you can use your own pretrained word embedding from previous homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_weights = [np.zeros(32)]\n",
    "for idx in range(1,len(idx_to_word)+1):\n",
    "    if(idx_to_word[idx] in embeddings.keys()):\n",
    "        embedding_weights.append(embeddings[idx_to_word[idx]])\n",
    "    else:\n",
    "        embedding_weights.append(np.zeros(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 102, 32)           480608    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 102, 64)           12480     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 102, 64)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 102, 48)           3120      \n",
      "=================================================================\n",
      "Total params: 496,208\n",
      "Trainable params: 15,600\n",
      "Non-trainable params: 480,608\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_to_idx),32,input_length=102,mask_zero=True,weights=[np.array(embedding_weights)],trainable=False))\n",
    "model.add(Bidirectional(GRU(32, return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(TimeDistributed(Dense(n_classes,activation='softmax')))\n",
    "model.summary()\n",
    "adam  = Adam(lr=0.001)\n",
    "model.compile(optimizer=adam,  loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18500/18500 [==============================] - 127s - loss: 2.4323 - categorical_accuracy: 0.3025   \n",
      "Epoch 2/10\n",
      "18500/18500 [==============================] - 123s - loss: 2.0896 - categorical_accuracy: 0.3640   \n",
      "Epoch 3/10\n",
      "18500/18500 [==============================] - 124s - loss: 1.9029 - categorical_accuracy: 0.4415   \n",
      "Epoch 4/10\n",
      "18500/18500 [==============================] - 121s - loss: 1.7234 - categorical_accuracy: 0.5017   \n",
      "Epoch 5/10\n",
      "18500/18500 [==============================] - 118s - loss: 1.6119 - categorical_accuracy: 0.5356   \n",
      "Epoch 6/10\n",
      "18500/18500 [==============================] - 117s - loss: 1.5339 - categorical_accuracy: 0.5582   \n",
      "Epoch 7/10\n",
      "18500/18500 [==============================] - 115s - loss: 1.4739 - categorical_accuracy: 0.5759   \n",
      "Epoch 8/10\n",
      "18500/18500 [==============================] - 114s - loss: 1.4238 - categorical_accuracy: 0.5914   \n",
      "Epoch 9/10\n",
      "18500/18500 [==============================] - 114s - loss: 1.3788 - categorical_accuracy: 0.6070   \n",
      "Epoch 10/10\n",
      "18500/18500 [==============================] - 114s - loss: 1.3401 - categorical_accuracy: 0.6207   \n",
      "CPU times: user 54min 27s, sys: 8min 58s, total: 1h 3min 25s\n",
      "Wall time: 19min 53s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faad18d3908>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(x_train,y_train,batch_size=64,epochs=10,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f_score</th>\n",
       "      <th>correct_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>93.5969</td>\n",
       "      <td>99.5658</td>\n",
       "      <td>96.4892</td>\n",
       "      <td>3669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>63.6856</td>\n",
       "      <td>65.6159</td>\n",
       "      <td>64.6363</td>\n",
       "      <td>5412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>53.5991</td>\n",
       "      <td>66.3982</td>\n",
       "      <td>59.3161</td>\n",
       "      <td>11214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>62.4492</td>\n",
       "      <td>84.3755</td>\n",
       "      <td>71.7751</td>\n",
       "      <td>10903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>54.5455</td>\n",
       "      <td>2.29885</td>\n",
       "      <td>4.41176</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>93.5414</td>\n",
       "      <td>83.5979</td>\n",
       "      <td>88.2906</td>\n",
       "      <td>1738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>49.2063</td>\n",
       "      <td>7.46988</td>\n",
       "      <td>12.9707</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>17.9487</td>\n",
       "      <td>1.90217</td>\n",
       "      <td>3.4398</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>100</td>\n",
       "      <td>22.093</td>\n",
       "      <td>36.1905</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>73.197</td>\n",
       "      <td>84.8939</td>\n",
       "      <td>78.6127</td>\n",
       "      <td>680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>55.5668</td>\n",
       "      <td>45.8264</td>\n",
       "      <td>50.2287</td>\n",
       "      <td>1647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>73.4664</td>\n",
       "      <td>73.7475</td>\n",
       "      <td>73.6067</td>\n",
       "      <td>4048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>46.2963</td>\n",
       "      <td>4.45633</td>\n",
       "      <td>8.13008</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>55.7228</td>\n",
       "      <td>50.8743</td>\n",
       "      <td>53.1882</td>\n",
       "      <td>1222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>73.822</td>\n",
       "      <td>24.1438</td>\n",
       "      <td>36.3871</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>72.427</td>\n",
       "      <td>81.645</td>\n",
       "      <td>76.7603</td>\n",
       "      <td>943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>98.1132</td>\n",
       "      <td>15.0725</td>\n",
       "      <td>26.1307</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>93.9346</td>\n",
       "      <td>77.6898</td>\n",
       "      <td>85.0434</td>\n",
       "      <td>1177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>49.4949</td>\n",
       "      <td>6.11354</td>\n",
       "      <td>10.8828</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>56.4854</td>\n",
       "      <td>47.8384</td>\n",
       "      <td>51.8035</td>\n",
       "      <td>675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>69.5652</td>\n",
       "      <td>38.4699</td>\n",
       "      <td>49.5426</td>\n",
       "      <td>352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>58.9744</td>\n",
       "      <td>5.56901</td>\n",
       "      <td>10.177</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>100</td>\n",
       "      <td>1.13636</td>\n",
       "      <td>2.24719</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>29</td>\n",
       "      <td>85.6061</td>\n",
       "      <td>35.7595</td>\n",
       "      <td>50.4464</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>30</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>31</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>32</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>33</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>34</td>\n",
       "      <td>83.299</td>\n",
       "      <td>71.8861</td>\n",
       "      <td>77.1729</td>\n",
       "      <td>404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>35</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>36</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>37</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>38</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>39</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>40</td>\n",
       "      <td>92.6966</td>\n",
       "      <td>58.9286</td>\n",
       "      <td>72.0524</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>41</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>42</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>43</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>45</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>46</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>accuracy=63.96</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tag precision   recall  f_score correct_count\n",
       "0                1   93.5969  99.5658  96.4892          3669\n",
       "1                2   63.6856  65.6159  64.6363          5412\n",
       "2                3   53.5991  66.3982  59.3161         11214\n",
       "3                4   62.4492  84.3755  71.7751         10903\n",
       "4                5         -        0        -             0\n",
       "5                6   54.5455  2.29885  4.41176            12\n",
       "6                7   93.5414  83.5979  88.2906          1738\n",
       "7                8   49.2063  7.46988  12.9707            31\n",
       "8                9   17.9487  1.90217   3.4398             7\n",
       "9               10         -        0        -             0\n",
       "10              11       100   22.093  36.1905            19\n",
       "11              12    73.197  84.8939  78.6127           680\n",
       "12              13   55.5668  45.8264  50.2287          1647\n",
       "13              14   73.4664  73.7475  73.6067          4048\n",
       "14              15   46.2963  4.45633  8.13008            50\n",
       "15              16   55.7228  50.8743  53.1882          1222\n",
       "16              17    73.822  24.1438  36.3871           141\n",
       "17              18    72.427   81.645  76.7603           943\n",
       "18              19   98.1132  15.0725  26.1307            52\n",
       "19              20         -        0        -             0\n",
       "20              21   93.9346  77.6898  85.0434          1177\n",
       "21              22   49.4949  6.11354  10.8828            98\n",
       "22              23   56.4854  47.8384  51.8035           675\n",
       "23              24   69.5652  38.4699  49.5426           352\n",
       "24              25   58.9744  5.56901   10.177            23\n",
       "25              26       100  1.13636  2.24719             2\n",
       "26              27         -        0        -             0\n",
       "27              29   85.6061  35.7595  50.4464           113\n",
       "28              30         -        0        -             0\n",
       "29              31         -        0        -             0\n",
       "30              32         -        0        -             0\n",
       "31              33         -        0        -             0\n",
       "32              34    83.299  71.8861  77.1729           404\n",
       "33              35         -        0        -             0\n",
       "34              36         -        0        -             0\n",
       "35              37         -        0        -             0\n",
       "36              38         -        0        -             0\n",
       "37              39         -        0        -             0\n",
       "38              40   92.6966  58.9286  72.0524           165\n",
       "39              41         -        0        -             0\n",
       "40              42         -        0        -             0\n",
       "41              43         -        0        -             0\n",
       "42              45         -        0        -             0\n",
       "43              46         -        0        -             0\n",
       "44  accuracy=63.96                                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 42.6 s, sys: 6.91 s, total: 49.5 s\n",
      "Wall time: 17.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.save_weights('/data/fixw_pos_no_crf.h5')\n",
    "y_pred=model.predict(x_test)\n",
    "ypred = [outputToLabel(y_pred[i],len(y_test[i])) for i in range(len(y_pred))]\n",
    "evaluation_report(y_test, ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.3 Neural POS Tagger - Trainable pretrained weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #TODO 2\n",
    "We would like you create a neural postagger model with keras with the pretrained word embedding as an input. However The word embedding is trainable (not fixed). To finish this excercise you must train the model and show the evaluation report with this model as shown in the example.\n",
    "\n",
    "Please note that the given pretrained word embedding only have weights for the vocabuary in BEST corpus from previous homework.\n",
    "\n",
    "Optionally, you can use your own pretrained word embedding from previous homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 102, 32)           480608    \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 102, 64)           12480     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 102, 64)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 102, 48)           3120      \n",
      "=================================================================\n",
      "Total params: 496,208\n",
      "Trainable params: 496,208\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_to_idx),32,input_length=102,mask_zero=True,weights=[np.array(embedding_weights)],trainable=True))\n",
    "model.add(Bidirectional(GRU(32, return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(TimeDistributed(Dense(n_classes,activation='softmax')))\n",
    "model.summary()\n",
    "adam  = Adam(lr=0.001)\n",
    "model.compile(optimizer=adam,  loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18500/18500 [==============================] - 115s - loss: 1.9925 - categorical_accuracy: 0.4931   \n",
      "Epoch 2/10\n",
      "18500/18500 [==============================] - 114s - loss: 0.6026 - categorical_accuracy: 0.8652   \n",
      "Epoch 3/10\n",
      "18500/18500 [==============================] - 113s - loss: 0.3300 - categorical_accuracy: 0.9202   \n",
      "Epoch 4/10\n",
      "18500/18500 [==============================] - 114s - loss: 0.2420 - categorical_accuracy: 0.9392   \n",
      "Epoch 5/10\n",
      "18500/18500 [==============================] - 113s - loss: 0.2020 - categorical_accuracy: 0.9469   \n",
      "Epoch 6/10\n",
      "18500/18500 [==============================] - 113s - loss: 0.1794 - categorical_accuracy: 0.9517   \n",
      "Epoch 7/10\n",
      "18500/18500 [==============================] - 113s - loss: 0.1639 - categorical_accuracy: 0.9549   \n",
      "Epoch 8/10\n",
      "18500/18500 [==============================] - 113s - loss: 0.1527 - categorical_accuracy: 0.9570   \n",
      "Epoch 9/10\n",
      "18500/18500 [==============================] - 113s - loss: 0.1439 - categorical_accuracy: 0.9587   \n",
      "Epoch 10/10\n",
      "18500/18500 [==============================] - 112s - loss: 0.1365 - categorical_accuracy: 0.9598   \n",
      "CPU times: user 51min 43s, sys: 8min 32s, total: 1h 15s\n",
      "Wall time: 18min 58s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faad18d36a0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(x_train,y_train,batch_size=64,epochs=10,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f_score</th>\n",
       "      <th>correct_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>99.431</td>\n",
       "      <td>99.5929</td>\n",
       "      <td>99.5119</td>\n",
       "      <td>3670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>94.648</td>\n",
       "      <td>95.1988</td>\n",
       "      <td>94.9226</td>\n",
       "      <td>7852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>91.382</td>\n",
       "      <td>96.9388</td>\n",
       "      <td>94.0784</td>\n",
       "      <td>16372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>99.9767</td>\n",
       "      <td>99.6208</td>\n",
       "      <td>99.7984</td>\n",
       "      <td>12873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>95.6522</td>\n",
       "      <td>98.5075</td>\n",
       "      <td>97.0588</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>99.7817</td>\n",
       "      <td>87.5479</td>\n",
       "      <td>93.2653</td>\n",
       "      <td>457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>97.2275</td>\n",
       "      <td>97.8355</td>\n",
       "      <td>97.5306</td>\n",
       "      <td>2034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>74.7292</td>\n",
       "      <td>49.8795</td>\n",
       "      <td>59.8266</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>76.4259</td>\n",
       "      <td>54.6196</td>\n",
       "      <td>63.7084</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>62.4765</td>\n",
       "      <td>39.6901</td>\n",
       "      <td>48.5423</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>93.6709</td>\n",
       "      <td>86.0465</td>\n",
       "      <td>89.697</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>97.7695</td>\n",
       "      <td>98.5019</td>\n",
       "      <td>98.1343</td>\n",
       "      <td>789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>89.7072</td>\n",
       "      <td>86.0879</td>\n",
       "      <td>87.8603</td>\n",
       "      <td>3094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>94.3716</td>\n",
       "      <td>94.3888</td>\n",
       "      <td>94.3802</td>\n",
       "      <td>5181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>81.5035</td>\n",
       "      <td>73.4403</td>\n",
       "      <td>77.2621</td>\n",
       "      <td>824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>88.1864</td>\n",
       "      <td>88.2598</td>\n",
       "      <td>88.2231</td>\n",
       "      <td>2120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>97.7901</td>\n",
       "      <td>90.9247</td>\n",
       "      <td>94.2325</td>\n",
       "      <td>531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>98.2005</td>\n",
       "      <td>99.2208</td>\n",
       "      <td>98.708</td>\n",
       "      <td>1146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>97.9412</td>\n",
       "      <td>96.5217</td>\n",
       "      <td>97.2263</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>98.2818</td>\n",
       "      <td>96.9492</td>\n",
       "      <td>97.6109</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>94.5491</td>\n",
       "      <td>92.7393</td>\n",
       "      <td>93.6355</td>\n",
       "      <td>1405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>84.6506</td>\n",
       "      <td>80.8484</td>\n",
       "      <td>82.7058</td>\n",
       "      <td>1296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>90.4064</td>\n",
       "      <td>96.1729</td>\n",
       "      <td>93.2005</td>\n",
       "      <td>1357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>86.4629</td>\n",
       "      <td>86.5574</td>\n",
       "      <td>86.5101</td>\n",
       "      <td>792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>89.9135</td>\n",
       "      <td>75.5448</td>\n",
       "      <td>82.1053</td>\n",
       "      <td>312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>93.1034</td>\n",
       "      <td>92.0455</td>\n",
       "      <td>92.5714</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>93.6937</td>\n",
       "      <td>79.3893</td>\n",
       "      <td>85.9504</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>29</td>\n",
       "      <td>92.1922</td>\n",
       "      <td>97.1519</td>\n",
       "      <td>94.6071</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>30</td>\n",
       "      <td>75.2475</td>\n",
       "      <td>74.5098</td>\n",
       "      <td>74.8768</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>31</td>\n",
       "      <td>67.3913</td>\n",
       "      <td>90.2913</td>\n",
       "      <td>77.1784</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>32</td>\n",
       "      <td>70.2128</td>\n",
       "      <td>55.618</td>\n",
       "      <td>62.069</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>33</td>\n",
       "      <td>87.5</td>\n",
       "      <td>51.4706</td>\n",
       "      <td>64.8148</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>34</td>\n",
       "      <td>92.3775</td>\n",
       "      <td>90.5694</td>\n",
       "      <td>91.4645</td>\n",
       "      <td>509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>35</td>\n",
       "      <td>83.3333</td>\n",
       "      <td>55.5556</td>\n",
       "      <td>66.6667</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>36</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>37</td>\n",
       "      <td>90.991</td>\n",
       "      <td>99.0196</td>\n",
       "      <td>94.8357</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>38</td>\n",
       "      <td>63.3333</td>\n",
       "      <td>48.7179</td>\n",
       "      <td>55.0725</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>39</td>\n",
       "      <td>81.6901</td>\n",
       "      <td>82.8571</td>\n",
       "      <td>82.2695</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>40</td>\n",
       "      <td>100</td>\n",
       "      <td>99.6429</td>\n",
       "      <td>99.8211</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>41</td>\n",
       "      <td>71.4286</td>\n",
       "      <td>75</td>\n",
       "      <td>73.1707</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>42</td>\n",
       "      <td>100</td>\n",
       "      <td>64.7059</td>\n",
       "      <td>78.5714</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>43</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>45</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>46</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>accuracy=93.59</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tag precision   recall  f_score correct_count\n",
       "0                1    99.431  99.5929  99.5119          3670\n",
       "1                2    94.648  95.1988  94.9226          7852\n",
       "2                3    91.382  96.9388  94.0784         16372\n",
       "3                4   99.9767  99.6208  99.7984         12873\n",
       "4                5   95.6522  98.5075  97.0588            66\n",
       "5                6   99.7817  87.5479  93.2653           457\n",
       "6                7   97.2275  97.8355  97.5306          2034\n",
       "7                8   74.7292  49.8795  59.8266           207\n",
       "8                9   76.4259  54.6196  63.7084           201\n",
       "9               10   62.4765  39.6901  48.5423           333\n",
       "10              11   93.6709  86.0465   89.697            74\n",
       "11              12   97.7695  98.5019  98.1343           789\n",
       "12              13   89.7072  86.0879  87.8603          3094\n",
       "13              14   94.3716  94.3888  94.3802          5181\n",
       "14              15   81.5035  73.4403  77.2621           824\n",
       "15              16   88.1864  88.2598  88.2231          2120\n",
       "16              17   97.7901  90.9247  94.2325           531\n",
       "17              18   98.2005  99.2208   98.708          1146\n",
       "18              19   97.9412  96.5217  97.2263           333\n",
       "19              20   98.2818  96.9492  97.6109           286\n",
       "20              21   94.5491  92.7393  93.6355          1405\n",
       "21              22   84.6506  80.8484  82.7058          1296\n",
       "22              23   90.4064  96.1729  93.2005          1357\n",
       "23              24   86.4629  86.5574  86.5101           792\n",
       "24              25   89.9135  75.5448  82.1053           312\n",
       "25              26   93.1034  92.0455  92.5714           162\n",
       "26              27   93.6937  79.3893  85.9504           104\n",
       "27              29   92.1922  97.1519  94.6071           307\n",
       "28              30   75.2475  74.5098  74.8768            76\n",
       "29              31   67.3913  90.2913  77.1784            93\n",
       "30              32   70.2128   55.618   62.069            99\n",
       "31              33      87.5  51.4706  64.8148            35\n",
       "32              34   92.3775  90.5694  91.4645           509\n",
       "33              35   83.3333  55.5556  66.6667             5\n",
       "34              36       100      100      100            16\n",
       "35              37    90.991  99.0196  94.8357           101\n",
       "36              38   63.3333  48.7179  55.0725            19\n",
       "37              39   81.6901  82.8571  82.2695           116\n",
       "38              40       100  99.6429  99.8211           279\n",
       "39              41   71.4286       75  73.1707            15\n",
       "40              42       100  64.7059  78.5714            11\n",
       "41              43         -        0        -             0\n",
       "42              45         -        0        -             0\n",
       "43              46         -        0        -             0\n",
       "44  accuracy=93.59                                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 42.1 s, sys: 6.91 s, total: 49 s\n",
      "Wall time: 17.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.save_weights('/data/nfixw_pos_no_crf.h5')\n",
    "y_pred=model.predict(x_test)\n",
    "ypred = [outputToLabel(y_pred[i],len(y_test[i])) for i in range(len(y_pred))]\n",
    "evaluation_report(y_test, ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #TODO 3\n",
    "Compare the result between all neural tagger models in 4.1.x and provide a convincing reason and example for the result of these models (which model perform best or worst, why?)\n",
    "\n",
    "(If you use your own weight please state so in the answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Write your answer here :</b>\n",
    "Model with fixed weights give bad result while model with trainable weight give lot better result because it model with fixed weights cannot adapt to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.1 CRF Viterbi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your next two tasks are to incorporate Conditional random fields (CRF) to your model. <b>You do not need to use pretrained weight</b>.\n",
    "\n",
    "Keras already implement a CRF neural model for you. However, you need to use the official extension repository for Keras library, call keras-contrib. You should read about keras-contrib crf layer before attempt this exercise section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #TODO 4\n",
    "Use Keras-contrib CRF layer in your model. You should set the layer parameter so it can give the best performance on testing using <b>viterbi algorithm</b>. Your model must use crf for loss function and metric. CRF is quite complex compare to previous example model, so you should train it with more epoch, so it can converge.\n",
    "\n",
    "To finish this excercise you must train the model and show the evaluation report with this model as shown in the example.\n",
    "\n",
    "Do not forget to save this model weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 102, 32)           480608    \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 102, 64)           12480     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 102, 64)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 102, 48)           3120      \n",
      "_________________________________________________________________\n",
      "crf_2 (CRF)                  (None, 102, 48)           4752      \n",
      "=================================================================\n",
      "Total params: 500,960\n",
      "Trainable params: 500,960\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "from keras_contrib.layers import CRF\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras import regularizers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_to_idx),32,input_length=102,mask_zero=True))\n",
    "model.add(Bidirectional(GRU(32, return_sequences=True)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(TimeDistributed(Dense(n_classes, activation='tanh')))\n",
    "crf = CRF(n_classes)\n",
    "model.add(crf)\n",
    "model.summary()\n",
    "adam  = Adam(lr=0.001)\n",
    "model.compile(optimizer=adam,loss=crf.loss_function, metrics=[crf.accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "18500/18500 [==============================] - 77s - loss: 45.4522 - acc: 0.7284    \n",
      "Epoch 2/20\n",
      "18500/18500 [==============================] - 76s - loss: 44.7925 - acc: 0.8942    \n",
      "Epoch 3/20\n",
      "18500/18500 [==============================] - 76s - loss: 44.6527 - acc: 0.9215    \n",
      "Epoch 4/20\n",
      "18500/18500 [==============================] - 77s - loss: 44.5964 - acc: 0.9318    \n",
      "Epoch 5/20\n",
      "18500/18500 [==============================] - 78s - loss: 44.5657 - acc: 0.9375    \n",
      "Epoch 6/20\n",
      "18500/18500 [==============================] - 81s - loss: 44.5454 - acc: 0.9411    \n",
      "Epoch 7/20\n",
      "18500/18500 [==============================] - 83s - loss: 44.5304 - acc: 0.9434    \n",
      "Epoch 8/20\n",
      "18500/18500 [==============================] - 83s - loss: 44.5193 - acc: 0.9465    \n",
      "Epoch 9/20\n",
      "18500/18500 [==============================] - 83s - loss: 44.5099 - acc: 0.9486    \n",
      "Epoch 10/20\n",
      "18500/18500 [==============================] - 83s - loss: 44.5024 - acc: 0.9500    \n",
      "Epoch 11/20\n",
      "18500/18500 [==============================] - 83s - loss: 44.4961 - acc: 0.9519    \n",
      "Epoch 12/20\n",
      "18500/18500 [==============================] - 83s - loss: 44.4891 - acc: 0.9534    \n",
      "Epoch 13/20\n",
      "18500/18500 [==============================] - 83s - loss: 44.4830 - acc: 0.9551    \n",
      "Epoch 14/20\n",
      "18500/18500 [==============================] - 83s - loss: 44.4783 - acc: 0.9560    \n",
      "Epoch 15/20\n",
      "18500/18500 [==============================] - 83s - loss: 44.4728 - acc: 0.9577    \n",
      "Epoch 16/20\n",
      "18500/18500 [==============================] - 83s - loss: 44.4682 - acc: 0.9585    \n",
      "Epoch 17/20\n",
      "18500/18500 [==============================] - 83s - loss: 44.4657 - acc: 0.9593    \n",
      "Epoch 18/20\n",
      "18500/18500 [==============================] - 83s - loss: 44.4604 - acc: 0.9606    \n",
      "Epoch 19/20\n",
      "18500/18500 [==============================] - 83s - loss: 44.4569 - acc: 0.9617    \n",
      "Epoch 20/20\n",
      "18500/18500 [==============================] - 83s - loss: 44.4530 - acc: 0.9630    \n",
      "CPU times: user 1h 13min 20s, sys: 11min 8s, total: 1h 24min 28s\n",
      "Wall time: 27min 17s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faad05f1240>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(x_train,y_train,batch_size=128,epochs=20,verbose=1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f_score</th>\n",
       "      <th>correct_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>99.8911</td>\n",
       "      <td>99.5929</td>\n",
       "      <td>99.7418</td>\n",
       "      <td>3670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>91.9832</td>\n",
       "      <td>95.4292</td>\n",
       "      <td>93.6745</td>\n",
       "      <td>7871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>91.0914</td>\n",
       "      <td>95.4763</td>\n",
       "      <td>93.2323</td>\n",
       "      <td>16125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>99.9222</td>\n",
       "      <td>99.3886</td>\n",
       "      <td>99.6547</td>\n",
       "      <td>12843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>95.6522</td>\n",
       "      <td>98.5075</td>\n",
       "      <td>97.0588</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>100</td>\n",
       "      <td>87.5479</td>\n",
       "      <td>93.3606</td>\n",
       "      <td>457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>97.8271</td>\n",
       "      <td>97.4507</td>\n",
       "      <td>97.6386</td>\n",
       "      <td>2026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>66.0494</td>\n",
       "      <td>51.5663</td>\n",
       "      <td>57.9161</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>76.1905</td>\n",
       "      <td>60.8696</td>\n",
       "      <td>67.6737</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>63.0088</td>\n",
       "      <td>42.4315</td>\n",
       "      <td>50.7123</td>\n",
       "      <td>356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>90.3226</td>\n",
       "      <td>97.6744</td>\n",
       "      <td>93.8547</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>96.2103</td>\n",
       "      <td>98.2522</td>\n",
       "      <td>97.2205</td>\n",
       "      <td>787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>89.6521</td>\n",
       "      <td>84.6132</td>\n",
       "      <td>87.0598</td>\n",
       "      <td>3041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>94.5485</td>\n",
       "      <td>93.8422</td>\n",
       "      <td>94.194</td>\n",
       "      <td>5151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>79.8604</td>\n",
       "      <td>71.3904</td>\n",
       "      <td>75.3882</td>\n",
       "      <td>801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>88.2793</td>\n",
       "      <td>88.4263</td>\n",
       "      <td>88.3527</td>\n",
       "      <td>2124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>96.1679</td>\n",
       "      <td>90.2397</td>\n",
       "      <td>93.1095</td>\n",
       "      <td>527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>97.3662</td>\n",
       "      <td>99.2208</td>\n",
       "      <td>98.2847</td>\n",
       "      <td>1146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>97.3761</td>\n",
       "      <td>96.8116</td>\n",
       "      <td>97.093</td>\n",
       "      <td>334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>98.2877</td>\n",
       "      <td>97.2881</td>\n",
       "      <td>97.7853</td>\n",
       "      <td>287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>92.4292</td>\n",
       "      <td>92.6733</td>\n",
       "      <td>92.5511</td>\n",
       "      <td>1404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>85.1604</td>\n",
       "      <td>79.476</td>\n",
       "      <td>82.2201</td>\n",
       "      <td>1274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>88.8239</td>\n",
       "      <td>96.8816</td>\n",
       "      <td>92.678</td>\n",
       "      <td>1367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>88.9831</td>\n",
       "      <td>80.3279</td>\n",
       "      <td>84.4342</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>91.5033</td>\n",
       "      <td>67.7966</td>\n",
       "      <td>77.886</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>91.3793</td>\n",
       "      <td>90.3409</td>\n",
       "      <td>90.8571</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>91.0714</td>\n",
       "      <td>77.8626</td>\n",
       "      <td>83.9506</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>29</td>\n",
       "      <td>95.5272</td>\n",
       "      <td>94.6203</td>\n",
       "      <td>95.0715</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>30</td>\n",
       "      <td>79.7872</td>\n",
       "      <td>73.5294</td>\n",
       "      <td>76.5306</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>31</td>\n",
       "      <td>60.7843</td>\n",
       "      <td>90.2913</td>\n",
       "      <td>72.6563</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>32</td>\n",
       "      <td>72.3404</td>\n",
       "      <td>57.3034</td>\n",
       "      <td>63.9498</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>33</td>\n",
       "      <td>92.1053</td>\n",
       "      <td>51.4706</td>\n",
       "      <td>66.0377</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>34</td>\n",
       "      <td>86.0197</td>\n",
       "      <td>93.0605</td>\n",
       "      <td>89.4017</td>\n",
       "      <td>523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>35</td>\n",
       "      <td>71.4286</td>\n",
       "      <td>55.5556</td>\n",
       "      <td>62.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>36</td>\n",
       "      <td>100</td>\n",
       "      <td>81.25</td>\n",
       "      <td>89.6552</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>37</td>\n",
       "      <td>91.8182</td>\n",
       "      <td>99.0196</td>\n",
       "      <td>95.283</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>38</td>\n",
       "      <td>61.2903</td>\n",
       "      <td>48.7179</td>\n",
       "      <td>54.2857</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>39</td>\n",
       "      <td>79.2453</td>\n",
       "      <td>90</td>\n",
       "      <td>84.2809</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>40</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>41</td>\n",
       "      <td>88.2353</td>\n",
       "      <td>75</td>\n",
       "      <td>81.0811</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>42</td>\n",
       "      <td>100</td>\n",
       "      <td>88.2353</td>\n",
       "      <td>93.75</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>45</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>46</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>accuracy=93.03</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tag precision   recall  f_score correct_count\n",
       "0                1   99.8911  99.5929  99.7418          3670\n",
       "1                2   91.9832  95.4292  93.6745          7871\n",
       "2                3   91.0914  95.4763  93.2323         16125\n",
       "3                4   99.9222  99.3886  99.6547         12843\n",
       "4                5   95.6522  98.5075  97.0588            66\n",
       "5                6       100  87.5479  93.3606           457\n",
       "6                7   97.8271  97.4507  97.6386          2026\n",
       "7                8   66.0494  51.5663  57.9161           214\n",
       "8                9   76.1905  60.8696  67.6737           224\n",
       "9               10   63.0088  42.4315  50.7123           356\n",
       "10              11   90.3226  97.6744  93.8547            84\n",
       "11              12   96.2103  98.2522  97.2205           787\n",
       "12              13   89.6521  84.6132  87.0598          3041\n",
       "13              14   94.5485  93.8422   94.194          5151\n",
       "14              15   79.8604  71.3904  75.3882           801\n",
       "15              16   88.2793  88.4263  88.3527          2124\n",
       "16              17   96.1679  90.2397  93.1095           527\n",
       "17              18   97.3662  99.2208  98.2847          1146\n",
       "18              19   97.3761  96.8116   97.093           334\n",
       "19              20   98.2877  97.2881  97.7853           287\n",
       "20              21   92.4292  92.6733  92.5511          1404\n",
       "21              22   85.1604   79.476  82.2201          1274\n",
       "22              23   88.8239  96.8816   92.678          1367\n",
       "23              24   88.9831  80.3279  84.4342           735\n",
       "24              25   91.5033  67.7966   77.886           280\n",
       "25              26   91.3793  90.3409  90.8571           159\n",
       "26              27   91.0714  77.8626  83.9506           102\n",
       "27              29   95.5272  94.6203  95.0715           299\n",
       "28              30   79.7872  73.5294  76.5306            75\n",
       "29              31   60.7843  90.2913  72.6563            93\n",
       "30              32   72.3404  57.3034  63.9498           102\n",
       "31              33   92.1053  51.4706  66.0377            35\n",
       "32              34   86.0197  93.0605  89.4017           523\n",
       "33              35   71.4286  55.5556     62.5             5\n",
       "34              36       100    81.25  89.6552            13\n",
       "35              37   91.8182  99.0196   95.283           101\n",
       "36              38   61.2903  48.7179  54.2857            19\n",
       "37              39   79.2453       90  84.2809           126\n",
       "38              40       100      100      100           280\n",
       "39              41   88.2353       75  81.0811            15\n",
       "40              42       100  88.2353    93.75            15\n",
       "41              43         0        0        -             0\n",
       "42              45         -        0        -             0\n",
       "43              46         -        0        -             0\n",
       "44  accuracy=93.03                                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 5s, sys: 10.7 s, total: 1min 16s\n",
      "Wall time: 26.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.save_weights('/data/crf_viterbi.h5')\n",
    "y_pred=model.predict(x_test)\n",
    "ypred = [outputToLabel(y_pred[i],len(y_test[i])) for i in range(len(y_pred))]\n",
    "evaluation_report(y_test, ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.2 CRF Marginal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #TODO 5\n",
    "\n",
    "Use Keras-contrib CRF layer in your model. You should set the layer parameter so it can give the best performance on testing using <b>marginal problabilities</b>. You <b>must not train the model</b> from scratch but use the pretrained weight from previous CRF Viterbi model.\n",
    "\n",
    "To finish this excercise you must train the model and show the evaluation report with this model as shown in the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 102, 32)           480608    \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 102, 64)           12480     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 102, 64)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 102, 48)           3120      \n",
      "_________________________________________________________________\n",
      "crf_3 (CRF)                  (None, 102, 48)           4752      \n",
      "=================================================================\n",
      "Total params: 500,960\n",
      "Trainable params: 500,960\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "from keras_contrib.layers import CRF\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras import regularizers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_to_idx),32,input_length=102,mask_zero=True))\n",
    "model.add(Bidirectional(GRU(32, return_sequences=True)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(TimeDistributed(Dense(n_classes, activation='tanh')))\n",
    "crf = CRF(n_classes,learn_mode='marginal',test_mode='marginal')\n",
    "model.add(crf)\n",
    "model.summary()\n",
    "adam  = Adam(lr=0.001)\n",
    "model.compile(optimizer=adam,loss=crf.loss_function, metrics=[crf.accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "18500/18500 [==============================] - 93s - loss: 0.1577 - acc: 0.9557    \n",
      "Epoch 2/20\n",
      "18500/18500 [==============================] - 90s - loss: 0.1440 - acc: 0.9593    \n",
      "Epoch 3/20\n",
      "18500/18500 [==============================] - 90s - loss: 0.1352 - acc: 0.9615    \n",
      "Epoch 4/20\n",
      "18500/18500 [==============================] - 89s - loss: 0.1305 - acc: 0.9625    \n",
      "Epoch 5/20\n",
      "18500/18500 [==============================] - 89s - loss: 0.1251 - acc: 0.9641    \n",
      "Epoch 6/20\n",
      "18500/18500 [==============================] - 89s - loss: 0.1225 - acc: 0.9648    \n",
      "Epoch 7/20\n",
      "18500/18500 [==============================] - 88s - loss: 0.1177 - acc: 0.9664    \n",
      "Epoch 8/20\n",
      "18500/18500 [==============================] - 89s - loss: 0.1138 - acc: 0.9672    \n",
      "Epoch 9/20\n",
      "18500/18500 [==============================] - 88s - loss: 0.1112 - acc: 0.9678    \n",
      "Epoch 10/20\n",
      "18500/18500 [==============================] - 89s - loss: 0.1094 - acc: 0.9685    \n",
      "Epoch 11/20\n",
      "18500/18500 [==============================] - 88s - loss: 0.1067 - acc: 0.9691    \n",
      "Epoch 12/20\n",
      "18500/18500 [==============================] - 88s - loss: 0.1033 - acc: 0.9704    \n",
      "Epoch 13/20\n",
      "18500/18500 [==============================] - 87s - loss: 0.1009 - acc: 0.9708    \n",
      "Epoch 14/20\n",
      "18500/18500 [==============================] - 88s - loss: 0.0986 - acc: 0.9715    \n",
      "Epoch 15/20\n",
      "18500/18500 [==============================] - 88s - loss: 0.0964 - acc: 0.9714    \n",
      "Epoch 16/20\n",
      "18500/18500 [==============================] - 88s - loss: 0.0946 - acc: 0.9723    \n",
      "Epoch 17/20\n",
      "18500/18500 [==============================] - 89s - loss: 0.0920 - acc: 0.9734    \n",
      "Epoch 18/20\n",
      "18500/18500 [==============================] - 93s - loss: 0.0899 - acc: 0.9740    \n",
      "Epoch 19/20\n",
      "18500/18500 [==============================] - 93s - loss: 0.0889 - acc: 0.9741    \n",
      "Epoch 20/20\n",
      "18500/18500 [==============================] - 93s - loss: 0.0867 - acc: 0.9746    \n",
      "CPU times: user 1h 16min 29s, sys: 12min 26s, total: 1h 28min 56s\n",
      "Wall time: 30min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.load_weights('/data/crf_viterbi.h5')\n",
    "model.fit(x_train,y_train,batch_size=128,epochs=20,verbose=1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f_score</th>\n",
       "      <th>correct_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>99.8641</td>\n",
       "      <td>99.7015</td>\n",
       "      <td>99.7827</td>\n",
       "      <td>3674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>94.1623</td>\n",
       "      <td>93.2832</td>\n",
       "      <td>93.7207</td>\n",
       "      <td>7694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>90.3477</td>\n",
       "      <td>95.9915</td>\n",
       "      <td>93.0841</td>\n",
       "      <td>16212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>99.3964</td>\n",
       "      <td>99.6973</td>\n",
       "      <td>12844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>95.6522</td>\n",
       "      <td>98.5075</td>\n",
       "      <td>97.0588</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>100</td>\n",
       "      <td>90.2299</td>\n",
       "      <td>94.864</td>\n",
       "      <td>471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>98.2985</td>\n",
       "      <td>97.2583</td>\n",
       "      <td>97.7756</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>56.3084</td>\n",
       "      <td>58.0723</td>\n",
       "      <td>57.1767</td>\n",
       "      <td>241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>76.087</td>\n",
       "      <td>57.0652</td>\n",
       "      <td>65.2174</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>61.6838</td>\n",
       "      <td>42.789</td>\n",
       "      <td>50.5278</td>\n",
       "      <td>359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>92.9577</td>\n",
       "      <td>76.7442</td>\n",
       "      <td>84.0764</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>94.7304</td>\n",
       "      <td>96.5044</td>\n",
       "      <td>95.6092</td>\n",
       "      <td>773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>86.2059</td>\n",
       "      <td>85.7262</td>\n",
       "      <td>85.9654</td>\n",
       "      <td>3081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>92.5138</td>\n",
       "      <td>94.3341</td>\n",
       "      <td>93.4151</td>\n",
       "      <td>5178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>81.5433</td>\n",
       "      <td>69.697</td>\n",
       "      <td>75.1562</td>\n",
       "      <td>782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>88.7746</td>\n",
       "      <td>86.2614</td>\n",
       "      <td>87.5</td>\n",
       "      <td>2072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>96.7626</td>\n",
       "      <td>92.1233</td>\n",
       "      <td>94.386</td>\n",
       "      <td>538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>97.8559</td>\n",
       "      <td>98.7879</td>\n",
       "      <td>98.3197</td>\n",
       "      <td>1141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>93.7853</td>\n",
       "      <td>96.2319</td>\n",
       "      <td>94.9928</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>97.5862</td>\n",
       "      <td>95.9322</td>\n",
       "      <td>96.7521</td>\n",
       "      <td>283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>94.3559</td>\n",
       "      <td>93.7954</td>\n",
       "      <td>94.0748</td>\n",
       "      <td>1421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>82.3529</td>\n",
       "      <td>80.3493</td>\n",
       "      <td>81.3388</td>\n",
       "      <td>1288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>90.2204</td>\n",
       "      <td>92.842</td>\n",
       "      <td>91.5124</td>\n",
       "      <td>1310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>92.5</td>\n",
       "      <td>72.7869</td>\n",
       "      <td>81.4679</td>\n",
       "      <td>666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>91.9014</td>\n",
       "      <td>63.1961</td>\n",
       "      <td>74.8924</td>\n",
       "      <td>261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>90.8571</td>\n",
       "      <td>90.3409</td>\n",
       "      <td>90.5983</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>89.7436</td>\n",
       "      <td>80.1527</td>\n",
       "      <td>84.6774</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>29</td>\n",
       "      <td>95</td>\n",
       "      <td>96.2025</td>\n",
       "      <td>95.5975</td>\n",
       "      <td>304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>30</td>\n",
       "      <td>73.0769</td>\n",
       "      <td>74.5098</td>\n",
       "      <td>73.7864</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>31</td>\n",
       "      <td>65.6489</td>\n",
       "      <td>83.4951</td>\n",
       "      <td>73.5043</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>32</td>\n",
       "      <td>62.8931</td>\n",
       "      <td>56.1798</td>\n",
       "      <td>59.3472</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>33</td>\n",
       "      <td>82.5</td>\n",
       "      <td>48.5294</td>\n",
       "      <td>61.1111</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>34</td>\n",
       "      <td>87.7797</td>\n",
       "      <td>90.7473</td>\n",
       "      <td>89.2388</td>\n",
       "      <td>510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>35</td>\n",
       "      <td>45.4545</td>\n",
       "      <td>55.5556</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>36</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>37</td>\n",
       "      <td>92.233</td>\n",
       "      <td>93.1373</td>\n",
       "      <td>92.6829</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>38</td>\n",
       "      <td>73.3333</td>\n",
       "      <td>56.4103</td>\n",
       "      <td>63.7681</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>39</td>\n",
       "      <td>76.4368</td>\n",
       "      <td>95</td>\n",
       "      <td>84.7134</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>40</td>\n",
       "      <td>100</td>\n",
       "      <td>99.6429</td>\n",
       "      <td>99.8211</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>41</td>\n",
       "      <td>88.8889</td>\n",
       "      <td>80</td>\n",
       "      <td>84.2105</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>42</td>\n",
       "      <td>100</td>\n",
       "      <td>82.3529</td>\n",
       "      <td>90.3226</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>43</td>\n",
       "      <td>50</td>\n",
       "      <td>11.1111</td>\n",
       "      <td>18.1818</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>45</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>46</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>accuracy=92.72</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tag precision   recall  f_score correct_count\n",
       "0                1   99.8641  99.7015  99.7827          3674\n",
       "1                2   94.1623  93.2832  93.7207          7694\n",
       "2                3   90.3477  95.9915  93.0841         16212\n",
       "3                4       100  99.3964  99.6973         12844\n",
       "4                5   95.6522  98.5075  97.0588            66\n",
       "5                6       100  90.2299   94.864           471\n",
       "6                7   98.2985  97.2583  97.7756          2022\n",
       "7                8   56.3084  58.0723  57.1767           241\n",
       "8                9    76.087  57.0652  65.2174           210\n",
       "9               10   61.6838   42.789  50.5278           359\n",
       "10              11   92.9577  76.7442  84.0764            66\n",
       "11              12   94.7304  96.5044  95.6092           773\n",
       "12              13   86.2059  85.7262  85.9654          3081\n",
       "13              14   92.5138  94.3341  93.4151          5178\n",
       "14              15   81.5433   69.697  75.1562           782\n",
       "15              16   88.7746  86.2614     87.5          2072\n",
       "16              17   96.7626  92.1233   94.386           538\n",
       "17              18   97.8559  98.7879  98.3197          1141\n",
       "18              19   93.7853  96.2319  94.9928           332\n",
       "19              20   97.5862  95.9322  96.7521           283\n",
       "20              21   94.3559  93.7954  94.0748          1421\n",
       "21              22   82.3529  80.3493  81.3388          1288\n",
       "22              23   90.2204   92.842  91.5124          1310\n",
       "23              24      92.5  72.7869  81.4679           666\n",
       "24              25   91.9014  63.1961  74.8924           261\n",
       "25              26   90.8571  90.3409  90.5983           159\n",
       "26              27   89.7436  80.1527  84.6774           105\n",
       "27              29        95  96.2025  95.5975           304\n",
       "28              30   73.0769  74.5098  73.7864            76\n",
       "29              31   65.6489  83.4951  73.5043            86\n",
       "30              32   62.8931  56.1798  59.3472           100\n",
       "31              33      82.5  48.5294  61.1111            33\n",
       "32              34   87.7797  90.7473  89.2388           510\n",
       "33              35   45.4545  55.5556       50             5\n",
       "34              36       100      100      100            16\n",
       "35              37    92.233  93.1373  92.6829            95\n",
       "36              38   73.3333  56.4103  63.7681            22\n",
       "37              39   76.4368       95  84.7134           133\n",
       "38              40       100  99.6429  99.8211           279\n",
       "39              41   88.8889       80  84.2105            16\n",
       "40              42       100  82.3529  90.3226            14\n",
       "41              43        50  11.1111  18.1818             1\n",
       "42              45         -        0        -             0\n",
       "43              46         -        0        -             0\n",
       "44  accuracy=92.72                                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 4s, sys: 11.9 s, total: 1min 16s\n",
      "Wall time: 27.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.save_weights('/data/crf_marginal.h5')\n",
    "y_pred=model.predict(x_test)\n",
    "ypred = [outputToLabel(y_pred[i],len(y_test[i])) for i in range(len(y_pred))]\n",
    "evaluation_report(y_test, ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #TODO 6\n",
    "\n",
    "Please pick the best example that can show the different between CRF that use viterbi and CRF that use marginal problabilities. Compare the result and provide a convincing reason. (which model perform better, why?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Write your answer here :</b>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
